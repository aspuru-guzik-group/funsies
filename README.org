*Funsies* is an opinionated typed python library to build /reproducible,
composable and data-persistent/ computational workflows that are described
entirely in Python. It uses the minimal queuing library [[https://python-rq.org/][rq]] and it's own
memoization / caching structures, both backed by [[https://redis.io/][an in-memory, persistent
redis server]].

* A little tour
** Building a workflow
Funsies is built to be simple to use. First, it needs a redis server, which
can be locally installed in anaconda,
#+BEGIN_SRC shell
  conda install -c anaconda redis
  redis-server
#+END_SRC
Then, the workflow can be built and registered in the Redis database entirely
in python,
#+BEGIN_SRC python
  from funsies import shell
  from redis import Redis

  db = Redis()

  # load some parameters
  with open('params.in','rb') as f:
      parameters = f.read

  # make the first job
  t1 = shell(db, "qchem params.in",
            inp={"file.in": parameters}, out=["params.out"])
#+END_SRC
~shell()~ make simple shell commands with explicit input and output files. All
input and output files are automatically saved in Redis.

Chaining shell commands is simple,
#+BEGIN_SRC python
  t2 = shell(
      db,
      ['grep "HOMO-LUMO ENERGY" myfile'],
      inp={"myfile": t1.out["params.out"]},
  )
#+END_SRC
The outputs from a call to ~shell()~ can immediately be used as inputs to other
tasks, even as none of the shell commands have even started computing. This is
because the "files" are not actual populated values but pointers to (currently
absent) data on the redis server.

Python can also be used to do computations. Some user-friendly wrappers are
provided in ~morph()~, ~reduce()~ and ~map()~. Here we apply ~a_python_function~
directly to the output of grep above,
#+BEGIN_SRC python
  def uncap(arg: bytes) -> bytes:
      return arg.decode().lower().encode()

  from funsies import morph, take
  t3 = morph(db, uncap, t2.stdout)
#+END_SRC
The result in ~t3~ is now in lowercase. Once the workflow has been executed, it
becomes retrievable using ~take().~

** Running a workflow
Running workflows is done using rq, a minimalist job queue library. To create
workers, simply run the following in the shell,
#+BEGIN_SRC shell
rq worker
#+END_SRC
There are many settings available. Importantly, workers can connect to remote
Redis servers, which allow full distributed computations. An example as to how
to do this on a SLURM server (common in HPC) is shown in
~examples/slurm_submit.sh~.

Once a workflow is setup in python, the ~execute~ function applied to any of the
workflow's object (from ~shell()~ etc.) will enqueue the entire DAG required to
compute the object. It's as simple as doing,
#+BEGIN_SRC python
  import rq
  queue = rq.Queue(connection=db)
  execute(db, queue, t3)
#+END_SRC
Note that no dependency that has already been computed is ever recomputed.
Thus, ~execute()~ can be used to re-run only data analysis routines (that change
often) without re-running any simulations.

** Results, memoization and persistence
The major advantage of using funsies is that it automatically and
transparently saves all *marked* input and output "files". This memoization
enables automatic checkpointing and incremental computing.

Following on the example above, re-running the same script, *even on a
different machine*, will not perform any computations (beyond database
lookups). Modifying the script and re-running it will only recompute changed
results. This means, for example, that if we want to change slightly the final
data outputs of an expensive computation, we can do so entirely out of the
cluster. We only ever need to carry around two files: the database dump and
the computation script. 

To go back to our example, we can get the result from ~t3~ by pulling it's
output file once the computation is done. For example, we could ~scp~ the
database ~dump.db~ file to a local machine, start redis, and re-run the entire
script with only this line added,
#+BEGIN_SRC python
  from funsies import take
  print(take(db, t3))
#+END_SRC
to print the result from ~t3~. If we additionally wanted to inspect the stdout
from t1, we could add this at the end,
#+BEGIN_SRC python
  print(pull_file(db, t1.stdout))
#+END_SRC
No expensive computations are performed in either case.

** Dashboard
Currently running jobs can be inspected using [[https://github.com/Parallels/rq-dashboard][rq-dashboard]]. For HPC, this most
readily done using a ssh tunnel. On a specific node with access to the Redis
server (~${cluster_node}~ below), run the dashboard using
#+BEGIN_SRC shell
  rq-dashboard -u redis://${redis_server_url}:$port
#+END_SRC
On the local machine, run
#+BEGIN_SRC shell
  ssh -N -f -L 9181:${cluster_node}:9181 ${cluster_address}
#+END_SRC
to tunnel to the dashboard. If everything worked, the dashboard should be
accessible using a browser pointed at address http://localhost:9181

* Why not /x/ ?
(where /x/ ∈ S, [[https://github.com/pditommaso/awesome-pipeline][awesome pipelining]] ∪ [[https://github.com/meirwah/awesome-workflow-engines][workflow codes]] ⊂ S)

I've created funsies because I wanted a pipelining code that is minimal,
typed, deployable on HPC resources (not dependent on docker, AWS, etc.) and
(most importantly) with *reproducible, persistent memoization*.

Funsies is specifically built for the kind of workflows common in
computational chemistry. It is most similar to [[https://github.com/grailbio/reflow][reflow]], albeit in python
instead of Go, and significantly simpler (and less robust / featureful of
course).
- *Single source of truth*: In funsies, the script that generates the data also
  describes the data. While keeping code and data tightly coupled is often
  frowned upon, it ensures that there is no documentation that will go out of
  date or lab notebooks that are more "post-it notes on a board" than
  "notebook".
- *Few but expensive*: Funsies assumes that tasks are few but that they are very
  expensive to compute. It is designed for workflow with 100s ⨉ 40 core hour
  jobs (like optimizing molecular geometries) not workflows with 100,000 ⨉ 10
  core second jobs, as may be present in large scale data analytics.
- *Run anywhere*: Academic research is always severely financially constrained,
  and computational chemistry software is often site-locked. Containerization
  (like Docker) is still slowly coming into the HPC sphere. Funsies is built
  so that it can run anywhere without root access.
- *Minimal setup and interface*: Although full-scale workflow software is
  obviously more robust, it is also much too cumbersome to setup. Similarly,
  extensive design of database schema is too unwieldy, even if it is by far
  the better solution. Funsies target instead the "file-driven databases" used
  by academics that rapidly become unreadable, non-backed up messes (~expt_2020/jun/ParameterSearch3/alpha=0point3.csv~)

* Architecture
** Hash-based graph
Funsies stores all shell commands and python functions as values in redis
store, with keys given by hashing a set of invariants. For commmand-line
tasks, these are:
- Input file hashes (unordered)
- Output file names (unordered)
- Shell commands

For python functions, the invariants are:
- Input file hashes
- Number of outputs
- The name of the function
(Although cloudpickle is used to call python functions, the function name is
used to generate the address hash, as the pickle form is python version
dependent.)

Files with explicitly given content are hashed based on this content, while
files generated as outputs to other commands are only hashed based on the hash
of the generator.

This structure is analoguous to that of a blockchain (but as a directed
acyclic graph). Like a blockchain, it has the advantage that any modification
to the chain is immediately and automatically detectable as it yields
completely different hashes for all descending "blocks". Using this
architecture, we get transparent caching and incremental recomputation of
tasks and their dependent tasks.

** No filesystem
Funsies "files" are always artefacts in the database. This abstraction is
enforced by running all commandline tasks entirely in temporary directories.
Any files not explicitly saved as an output is *deleted*.

This is obviously a very opinionated design choice, but it is also one that
enables the caching scheme used by funsies. Indeed, by completely removing any
direct file management, we can ensure that *all file-like objects* are accounted
for at every point in incremental calculations, with no side-effects. I should
note that this means that "restart" files must be explicitly accounted for by
the user.

By completely abstracting away the filesystem, we ensure that every generated
result is fully specified within the calculation workflow.




* Future work
This code works but it's not entirely production ready. This is a list of some
action items that still need to be implemented: 
- Include higher-order operations on "files" (joins / concats)
- Build a CLI interface to look at files and graphs.
- Add an elegant way to make dynamic workflow.
- Make directories processable.
